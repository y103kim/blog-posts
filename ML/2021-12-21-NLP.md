---
layout: post
category: languages
title: Natural Language Processing
tags: [NLP, ML]
---

<style>
  body{counter-reset: section 0}
  div.post-body h1:before{
    counter-increment: section;
    content: counter(section) ". ";
  }
</style>

# Natural Language Processing with Classification and Vector Spaces

### Week1: Sentiment Analysis with Logistic Regression

- Logistic Regression 적용해보기
  - tweet의 postiive, negative 판단하기
  - supervised model
- Bag of word, Document-Term Matrix
  - Sparse representation이 문제
  - 중요 단어, 순서, 관계 표현 못함
- 빈도수 합으로 표현
  - corpus내 모든 단어의 PosFreq, NegFreq 구함
  - sumPosFreq: 문장 내 단어들의 PosFreq 합
  - $X_m = [1, \sum_w PosFreq(w), \sum_w NegFreq(w)]$
- preprocessing
  - stopword: and, is, a, at 등 지우기
  - punctuation: 쉼표, 마침표 등 지우기
  - stemming: tune, tuned, tuning
  - lowercasing: 소문자로 바꾸기
- logistic regression
  - classify/predict: $h = sigmoid(\theta \cdot x)$
  - cost function: $J = \frac{-1}{m}(y^T \cdot \log{h} + (1-y)^T \cdot \log(1-h))$
  - update grad: $\theta = \theta - \frac{\alpha}{m} x^T \cdot (h-y)$
  - until cost is low enough

```py
import nltk                                # Python library for NLP
from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK
from nltk.corpus import stopwords          # module for stop words that come with NLTK
from nltk.stem import PorterStemmer        # module for stemming
from nltk.tokenize import TweetTokenizer   # module for tokenizing strings

tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)
stemmer = PorterStemmer() 

def process_tweet(tweet):
  tweet = re.sub(r'^RT[\s]+', '', tweet)
  tweet = re.sub(r'https?://[^\s\n\r]+', '', tweet)
  tweet = re.sub(r'#', '', tweet)
  tokens = tokenizer.tokenize(tweet)
  tokens = [t for t in tokens if not t in stopwords.words('english')]
  tokens = [t for t in tokens if not t in string.punctuation]
  tokens = [stemmer.stem(t) for t in tokens]
  return tokens

def gradientDescent(x, y, theta, alpha, num_iters):
    m = x.shape[0]
    for i in range(0, num_iters):
        z = np.dot(x, theta)
        h = sigmoid(z)
        J = (-1/m) *  (np.dot(y.T, np.log(h)) + np.dot((1-y).T, np.log(1-h)))
        theta -= (alpha / m) * np.dot(x.T, h - y)
    return float(J), theta
```

### Week2: Sentiment Analysis with Naïve Bayes

- 나이브 베이즈 분류기
  - 긍정일때 단어의 분포, 부정일때 단어의 분포 알고있음 = $P(w_i \vert pos)$
  - 문장이 등장했을때, 긍정인지 부정인지 알고싶음
  - 문장 = 단어 여러개
  - 즉 단어 여러개에 대해 긍정인지 부정인지 알고 싶음 = $P(neg \vert w_1 w_2 .. w_m)$
  - 단어 각각의 등장이 독립적이라고 가정함
  - $NaiveBayes = \prod_i \cfrac {P({w_i}\vert pos)}{P({w_i}\vert neg)}$
- 유도과정
  - 단어들의 곱사건 을 $W$라 할때 $W = \prod_i{w_i}$
  - $P(pos\vert W) = P(W\vert pos) \cfrac{P(pos)}{P(W)}$
  - $P(neg\vert W) = P(W\vert neg) \cfrac{P(neg)}{P(W)}$
  - $NaiveBayes = \cfrac{P(pos\vert W)}{P(neg\vert W)}$ 
  - 독립이면 $P(W\vert pos) = \prod_i P(w_i\vert pos)$
  - $NaiveBayes = \cfrac {P(pos)}{P(neg)} \prod_i \cfrac {P({w_i}\vert pos)}{P({w_i}\vert neg)}$
  - positive, negative 비율이 같다면 
  - $NaiveBayes = \prod_i \cfrac {P({w_i}\vert pos)}{P({w_i}\vert neg)}$
- laplacian smoothing
  - $P({w_i}\vert neg)$이 0이라면 0으로 나누게되어서 위 식 못씀
  - 때문에 공통적으로 분자 분모에 특정 값을 더해서 보정해줌
  - $P({w_i}\vert neg) = \cfrac{freq(w_i, neg) + 1}{N_{neg} + m}$
  - $m$은 $w_i$에서 가능한 $i$의 수이고, 때문에 $\sum_{i=1}^m P(w_i \vert neg) = 1$
- log likelihood
  - 여러번 나누는 floating point연산은 underbound가 날 가능성이 높음
  - 그러므로 log를 취해서 계산
  - $\lambda(w) = \log {\cfrac {P(w \vert pos)}{P(w \vert neg)}}$
  - 미리 람다를 계산해두고 나중에는 더하기만 하면 됨
- 나이브 베이즈 문제점들
  - 독립조건 맞추기 어려움
  - 비율이 비슷한 데이터셋은 많지 않음
  - 단어 순서를 무시함
- 그 밖에 고려할 점
  - punctuation 지우면서, 지워지는 이모티콘이 감정을 나타내기도 함
  - `not`등 stopword를 지우면서 의미가 바뀌기도 함
  - 반어, 모순 등을 고려할것

### Week3: Vector Space Models

- distance, cosine similarity
- word analogy test
  - `Seoul -> Korea` == `Paris -> France`
- PCA
  - 벡터를 더 낮은 차원으로 근사해 visualize하는데 사용
  - 자세한 내용에 대해서는 추가 학습이 필요

### Week4: Machine Translation and Document Search

- 기계번역
  - 두 언어에 대한 임베딩 X, Y가 주어지고
  - 두 언어의 단어들에 대한 매핑이 주어질 때
  - $XR = Y$를 만족하는 $R$을 찾기
- 행렬에 대한 회귀분석 하기
  - evaluate: $\| F G \|_F^2$
  - loss: $\| XR - Y \|_F^2$
  - gardient: $\frac{2}{m}(X^T(XR-Y))$
  - test: Locality Sensitive Hashing (LSH) + K-mean
- Locality Sensitive Hashing (LSH)
  - 랜덤한 여러개의 hyper-plane, 해당 평면들의 normal vector
  - norm-vec에 대한 내적값으로 bit-hash 
  - 같은 hash를 가지는 점에 대해서만 K-mean 적용
  - planes need to be considered: $N$ -> $N/(2^{\#plane})$