---
layout: post
category: languages
title: Natural Language Processing
tags: [NLP, ML]
---

<style>
  body{counter-reset: section 0}
  div.post-body h1:before{
    counter-increment: section;
    content: counter(section) ". ";
  }
</style>

# Natural Language Processing with Classification and Vector Spaces

### Week1: Sentiment Analysis with Logistic Regression

- Logistic Regression 적용해보기
  - tweet의 postiive, negative 판단하기
  - supervised model
- Bag of word, Document-Term Matrix
  - Sparse representation이 문제
  - 중요 단어, 순서, 관계 표현 못함
- 빈도수 합으로 표현
  - corpus내 모든 단어의 PosFreq, NegFreq 구함
  - sumPosFreq: 문장 내 단어들의 PosFreq 합
  - $X_m = [1, \sum_w PosFreq(w), \sum_w NegFreq(w)]$
- preprocessing
  - stopword: and, is, a, at 등 지우기
  - punctuation: 쉼표, 마침표 등 지우기
  - stemming: tune, tuned, tuning
  - lowercasing: 소문자로 바꾸기
- logistic regression
  - classify/predict: $h = sigmoid(\theta \cdot x)$
  - cost function: $J = \frac{-1}{m}(y^T \cdot \log{h} + (1-y)^T \cdot \log(1-h))$
  - update grad: $\theta = \theta - \frac{\alpha}{m} x^T \cdot (h-y)$
  - until cost is low enough

```py
import nltk                                # Python library for NLP
from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK
from nltk.corpus import stopwords          # module for stop words that come with NLTK
from nltk.stem import PorterStemmer        # module for stemming
from nltk.tokenize import TweetTokenizer   # module for tokenizing strings

tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)
stemmer = PorterStemmer() 

def process_tweet(tweet):
  tweet = re.sub(r'^RT[\s]+', '', tweet)
  tweet = re.sub(r'https?://[^\s\n\r]+', '', tweet)
  tweet = re.sub(r'#', '', tweet)
  tokens = tokenizer.tokenize(tweet)
  tokens = [t for t in tokens if not t in stopwords.words('english')]
  tokens = [t for t in tokens if not t in string.punctuation]
  tokens = [stemmer.stem(t) for t in tokens]
  return tokens

def gradientDescent(x, y, theta, alpha, num_iters):
    m = x.shape[0]
    for i in range(0, num_iters):
        z = np.dot(x, theta)
        h = sigmoid(z)
        J = (-1/m) *  (np.dot(y.T, np.log(h)) + np.dot((1-y).T, np.log(1-h)))
        theta -= (alpha / m) * np.dot(x.T, h - y)
    return float(J), theta
```

### Week2: Sentiment Analysis with Naïve Bayes

- 나이브 베이즈 분류기
  - 긍정일때 단어의 분포, 부정일때 단어의 분포 알고있음 = $P(w_i \vert pos)$
  - 문장이 등장했을때, 긍정인지 부정인지 알고싶음
  - 문장 = 단어 여러개
  - 즉 단어 여러개에 대해 긍정인지 부정인지 알고 싶음 = $P(neg \vert w_1 w_2 .. w_m)$
  - 단어 각각의 등장이 독립적이라고 가정함
  - $NaiveBayes = \prod_i \cfrac {P({w_i}\vert pos)}{P({w_i}\vert neg)}$
- 유도과정
  - 단어들의 곱사건 을 $W$라 할때 $W = \prod_i{w_i}$
  - $P(pos\vert W) = P(W\vert pos) \cfrac{P(pos)}{P(W)}$
  - $P(neg\vert W) = P(W\vert neg) \cfrac{P(neg)}{P(W)}$
  - $NaiveBayes = \cfrac{P(pos\vert W)}{P(neg\vert W)}$ 
  - 독립이면 $P(W\vert pos) = \prod_i P(w_i\vert pos)$
  - $NaiveBayes = \cfrac {P(pos)}{P(neg)} \prod_i \cfrac {P({w_i}\vert pos)}{P({w_i}\vert neg)}$
  - positive, negative 비율이 같다면 
  - $NaiveBayes = \prod_i \cfrac {P({w_i}\vert pos)}{P({w_i}\vert neg)}$
- laplacian smoothing
  - $P({w_i}\vert neg)$이 0이라면 0으로 나누게되어서 위 식 못씀
  - 때문에 공통적으로 분자 분모에 특정 값을 더해서 보정해줌
  - $P({w_i}\vert neg) = \cfrac{freq(w_i, neg) + 1}{N_{neg} + m}$
  - $m$은 $w_i$에서 가능한 $i$의 수이고, 때문에 $\sum_{i=1}^m P(w_i \vert neg) = 1$
- log likelihood
  - 여러번 나누는 floating point연산은 underbound가 날 가능성이 높음
  - 그러므로 log를 취해서 계산
  - $\lambda(w) = \log {\cfrac {P(w \vert pos)}{P(w \vert neg)}}$
  - 미리 람다를 계산해두고 나중에는 더하기만 하면 됨
- 나이브 베이즈 문제점들
  - 독립조건 맞추기 어려움
  - 비율이 비슷한 데이터셋은 많지 않음
  - 단어 순서를 무시함
- 그 밖에 고려할 점
  - punctuation 지우면서, 지워지는 이모티콘이 감정을 나타내기도 함
  - `not`등 stopword를 지우면서 의미가 바뀌기도 함
  - 반어, 모순 등을 고려할것

### Week3: Vector Space Models

- distance, cosine similarity
- word analogy test
  - `Seoul -> Korea` == `Paris -> France`
- PCA
  - 벡터를 더 낮은 차원으로 근사해 visualize하는데 사용
  - 자세한 내용에 대해서는 추가 학습이 필요

### Week4: Machine Translation and Document Search

- 기계번역
  - 두 언어에 대한 임베딩 X, Y가 주어지고
  - 두 언어의 단어들에 대한 매핑이 주어질 때
  - $XR = Y$를 만족하는 $R$을 찾기
- 행렬에 대한 회귀분석 하기
  - evaluate: $\| F G \|_F^2$
  - loss: $\| XR - Y \|_F^2$
  - gardient: $\frac{2}{m}(X^T(XR-Y))$
  - test: Locality Sensitive Hashing (LSH) + K-mean
- Locality Sensitive Hashing (LSH)
  - 랜덤한 여러개의 hyper-plane, 해당 평면들의 normal vector
  - norm-vec에 대한 내적값으로 bit-hash 
  - 같은 hash를 가지는 점에 대해서만 K-mean 적용
  - planes need to be considered: $N$ -> $N/(2^{plane count})$

# Natural Language Processing with Probabilistic Models

### Week1: Autocorrect

- edit distance 계산에 관한 내용
- 기본적인 DP만 알면 됨

### Week2: Part of Speech Tagging and Hidden Markov Models

- part of speech tagging: 품사, Named entitiy 추정
- Markov Assumption
  - 미래 상태는 현재 상태에만 영향을 받는다.
  - $P(S_{t+1} \vert S_1, ..., S_{t}) = P(S_{t+1} \vert S_t)$
- Markov Chain
  - 시스템을 상태의 전이로 표현
  - state transition prob matrix $A(a_{ij}) = P(S_j \vert S_i)$ 
  - state로 sequence 데이터를 표현할 수 있다.
- Hidden Markov Model $\lambda = (A, B, \pi)$
  - 두 sequence가 주어짐 (observable, hidden)
  - observable($O_k$)은 hidden($S_i$)에 종속적이라고 간주
  - state transition prob matrix $A(a_{ij} = P(S_{t+1} = s_j \vert S_{t} = s_i))$ 
  - emisstion prob matrix $B(b_{jk} = P(O_t = o_k \vert S_t = s_j))$
  - initial state prob matrix $\pi(\pi_{i} = P(S_1 = {s_i}))$ 
- HMM 문제(1) - Evaluation: O가 관측될 확률
  - Question: $P(O_1 = o_a, O_2 = o_b, O_3 = o_a)$ ?
  - 베이즈 공식을 쓰면, 계산량이 폭발함
  - $\sum_{i,j,k} P(o_a, o_b, o_a \vert s_i, s_j, s_k)$
  - forward algorithm (dynamic programming) 사용
  - $\alpha_1(i) = P(O_1=o_a, S_1=s_i) = \pi_i * b_{ia}$
  - $\alpha_2(i) = P(O_1=o_a, O_2=o_b, S_2=s_i) = [\sum_j \alpha_1(j)*a_{ji}]*b_{ib}$
  - 마지막 상태 은닉상태를 기준으로 종합하므로, 이후 상태 전이에 활용 가능해짐
- HMM 문제(2) - Decoding: O가 주어질때 가장 가능성있는 H를 찾기
  - viterbi algorithm 사용
  - forward algorithm의 sum 대신 argmax를 사용
  - 이전 상태의 최대값 기준으로 전이시켜 다시 최대값을 찾는 방법
  - $v_1(i) = \pi_i * b_{ia}$
  - $v_2(i) = [argmax_j v_1(j)*a_{ji}]*b_{ib}$
  - optimal pass를 찾기 위해 최대값을 찾았던 인덱스를 저장해둬야함
  - 이후 마지막 최대값으로 부터 인덱스 보면서 따라가면 됨
  - viterbi, forward 모두 로그를 취한 확률을 더하는게 좋음(underflow 방지)
- HMM 문제(2) - Learning: 파라미터 학습하기
  - Baum-Welch Algorithm 사용
  - 나중에 학습하기 [링크](https://youtu.be/P02Lws57gqM?t=2027)

### Week3: Autocomplete and Language Models

- N-gram: n개의 이전 단어를 가정하고, 현재 단어의 확률찾기
  - word sequence 문자($w_a^b$) = a에서서 b까지의 문자가 연속으로 등장할 확률
  - N-gram: $P(w_n \vert w_{n-1}^{n-N+1})$
  - word seq확률: $P(w_1^n) = P(w_1) \prod_{i=1}^n P(w_i \vert w_{i-1})$
- N-gram과 시작 문자, 종료 문자
  - 시작 문자 N-1개가 있으면 $P(w_1)$항을 없앨 수 있다.
  - $P(w_0^n) = \prod_{i=0}^n P(w_i \vert w_{i-1})$
  - 종료 문자가 있으면 길이가 다른 문장의 생성확률이 하나의 확률공간으로 묶인다.
- N-gram LM
  - N-gram은 카운트로 계산 가능
  - $b-a = n-1$일때 $P(w_b \vert w_a^{b-1}) = C(w_a^b)/C(w_a^{b-1})$
  - underflow 방지를 위해 로그 확률의 합으로 계산
  - 평가 지표는 Perplexity(lower is better) 사용
  - $logPP(W)= \displaystyle-\frac{1}{m}\sum_{i=1}^m \log_2 P(w_i \vert w_{i-1})$
- out-of-vocab word
  - vocab에 없는 단어는 기본적으로 확률계산 불가
  - `<UND>`로 변경해서 진행
  - 이 경우 Perplexity는 낮아지지만, 실제로 좋아지는건 아니다.
- Smoothing
  - vocab에 있는 문자도 분모나 분자가 0이 될수 있음
  - 이 경우 동일한 비율만큼 분모 분자에 더해서 0을 방지

### Week4: Word embeddings with neural networks

- word vector
  - Integer: (+)simple, (-)순서가 의미 없음
  - One-hot: (+)simple, (+)순서 내포 안함, (-)임베딩 함의 없음, (-)Sparse함
- basic word embedding
  - word2vec (Google, 2013) CBOW / Skip-gram
  - Global Vector, GloVe (Stanford, 2014)
  - fastText (Facebook, 2016), out-of-vocab word 지원
- advanced word embedding with DL
  - BERT(Google, 2018)
  - ELMo(Allen Institute for AI, 2018)
  - GPT-2(OpenAI, 2018)
- CBOW: Continuous bag-of-words
  - 중심어 주변으로 window size만큼 단어를 문맥으로 취급
  - context word vector -> center word vector
  - 기본적으로 one-hot으로 단어 벡터를 만든다.
  - context word vector = window내 단어들의 one-hot의 평균
  - V는 사전의 크기, N은 임베딩의 크기, B는 배치 크기
  - ctx(V) -> Dense1(N) -> ReLU -> Dense2(V) -> Softmax
  - (V,B) -> Dense1(N) -> (N,B) -> Dense2(V) -> (V,B)
  - 임베딩은 Dense1의 W1과 Dense2의 W2.T의 합으로 구함

# Natural Language Processing with Sequence Models

### Week1. Neural Networks for Sentiment Analysis

### Week2. Recurrent Neural Networks for Language Modeling

### Week3. LSTMs and Named Entity Recognition

### Week4. Siamese Networks

# Natural Language Processing with Attention Models

### Week1. Neural Machine Translation

### Week2. Text Summarization

### Week3. Question Answering

### Week4. Chatbot
