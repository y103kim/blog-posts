---
layout: post
title: 'Pytorch 정리'
tags: [ML, DL]
---

<https://deeplearningzerotoall.github.io/season2/>


# Tensor Manipulation

- numpy와 거의 같음, 단 `dtype`으로 `object`는 사용할 수 없음.
- 생성: 32bit-FP(`FloatTensor` 혹은 `cuda.FloatTensor`). 다른 타입도 많음
- `tensor.long()`처럼 타입 변환 가능
- 기본적으로 `axis`, `dim`은 가장 바깥쪽이 0, 안쪽이 -1임
- Broadcast는 numpy에서처럼, 차원을 채워줌, 직사각형 형태일때만 가능.
  - 사칙연산에서는 해줌
  - matmul에서는 안해줌, 차원이 맞아야함
- `sum`, `mean`, `max`, `min`에서 `dim` 사용 가능
- `max`, `min`에서는 두번째 항으로 arg의 위치가 나즉
- `view`는 numpy의 `reshape`와 유사함, 개수 기준으로 다른 모양으로 만듬
  - `[4,2,-1]` 처럼 -1을 적으면, 전체 개수를 8로 나눈 몫이 자동으로 들어옴
- `squeeze`는 없어져줌 무방한, 즉 1개만 들어있는 차원을 지워줌
- `unsqueeze`는 없어져도 무방한 차원을 만들어줌 `dim`을 줘야함
- `cat` 두 Tensor를 이어붙임, `dim`으로 지정한 위치에 차원을 뭉개고 합쳐줌
    ```py
    x = [[1],[2]]
    y = [[3],[4]]
    torch.cat(x,y,dim=0) == [[1],[2],[3],[4]]
    torch.cat(x,y,dim=1) == [[1,3],[2,4]]
    ```
- `stack` 두 Tensor를 이어붙임, `dim`으로 지정한 위치에 차원을 추가하면서 합쳐줌
    ```py
    a=[[0,1],[2,3]]
    b=[[4,5],[6,7]]
    torch.stack(a,b,dim=0) == [[[0,1],[2,3]],[[4,5],[6,7]]]
    torch.stack(a,b,dim=1) == [[[0,1],[4,5]],[[2,3],[6,7]]]
    # [[[0,1]],[[2,3]]] + [[[4,5]],[[6,7]]]
    torch.stack(a,b,dim=2) == [[[0,4],[1,5]],[[2,6],[3,7]]]
    ```
- `ones`는 1로 `zeros` 0으로, `ones_like`는 똑같은걸 1로, gpu/cpu 구분 필요할때 유용
- `tensor.mul`은 일반 곱하기, `tensor.mul_`은 `*=2`


# Linear Regression

- <https://github.com/y103kim/DL-practice/blob/main/pytorch/01-LinearRegression.ipynb>
- 선형 회귀는 $y = Wx + b$ 로 진행.
- 단일 변수이므로 1D Tensor
- cost 함수는 MSE(Mean Squared Error) 사용 `mean((H - y_train)**2)`임.
- `tensorboard`의 SummaryWriter를 쓰면 손쉽게 epoch당 변수의 변화를 추적 가능
- Optimizer를 정할 수 있음. 일단은 간단한 Optimizer 2개
  - *GD*: 전부다 기울기 강하
    - 강하 곡선이 스무스하다.
    - 벡터화로 인한, 캐시, 슈퍼스칼라, GPGPU 명령어 최적화 효과를 볼 수 있다.
    - 다만 한번 강하하는데 시간이 오래걸린다.
    - 때문에 수렴에까지 걸리는 전체 시간이 크다.
  - *SGD(Stochasitic Gradient Descent)*: mini-batch로 잘라서 기울기 강하
    - 강하 곡선이 마구 튄다. 확률적 수렴을 노린다.
    - mini-batch의 사이즈가 작으면, 백터화로 인한 최적화 효과가 줄어든다.
    - 한번 강하하는데 시간이 짧다
    - 다만 수렴하지 못할 가능성이 있다.
  - 그 이외에 Optimizer가 많고, 나중에 정리하자.
    - <https://seamless.tistory.com/38>
    - [The Marginal Value of Adaptive Gradient Methods](http://jaejunyoo.blogspot.com/2017/05/marginal-value-of-adaptive-gradient-methods-in-ML.html)


# Multivariable Linear regression

- <https://github.com/y103kim/DL-practice/blob/main/pytorch/02-Multivariable.ipynb>
- 기본적으로 벡터로 합쳐서 `matmul`돌리는것 이외에는 차이가 없다.
- nn.Linear를 쓰면 두가지가 짧아짐
  - $y = Wx + b$ 식을 쓰지 않아도 된다.
  - W와 b를 정의하지 않아도 된다.
  - Feature가 아주 많아질 경우 이미 정의된 모델을 이용해서 Graph를 만드는게 편하다.


# Dataset

- 데이터 세트의 종류에 따라 데이터를 추상화 할 수 있다.
  - `Dataset`은 `__getitem__`과 `__len__`을 구현하면 된다.
  - `IterableDataset`은 `__iter__`를 구현하면 된다.
  - `TensorDataset`은 tensor를 넣으면 되는데, 간단하게 쓰기에 좋을듯하다.